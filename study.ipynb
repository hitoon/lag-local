{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e492f5fd-b2fb-4d06-9d99-c2ef667e24d1",
   "metadata": {},
   "source": [
    "## 実験の目的\n",
    "- LLM(大規模言語モデル)が文脈に沿った回答を行えるようにすること\n",
    "- 今回は「スプラトゥーン3」に関する情報を回答できることを目指します\n",
    "\n",
    "\n",
    "\n",
    "## LangChainを利用\n",
    "- 今回の実験ではライブラリはLangChainを利用します\n",
    "- LangChainはLLM(大規模言語モデル)アプリケーションの開発のために便利な機能を提供してくれているツールです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e29353c-43c1-4f73-b861-95a9e88bb5bc",
   "metadata": {},
   "source": [
    "## RAGの3つのステップ\n",
    "\n",
    "1. **情報の準備 (データの読み込みとベクトル化)**\n",
    "テキストデータを分割し、ベクトル空間にマッピングします。\n",
    "2. **検索 (Retrieval)**\n",
    "クエリに基づいて関連するデータをベクトル空間から検索します。\n",
    "3. **生成 (Generation)**\n",
    "検索したデータを文脈(context)として使用し、生成AIが回答を生成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1618b7-0970-4203-9180-56aed53699ef",
   "metadata": {},
   "source": [
    "## 1. 情報の準備 (データの読み込みとベクトル化)\n",
    "\n",
    "### 生成AIに答えさせたい内容がまとまったデータを準備する\n",
    "- 今回はWikiの「スプラトゥーン3」のページを利用します\n",
    "- ページの取得は省略します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59e9288-fe55-4b81-9ef7-7b5610473b16",
   "metadata": {},
   "source": [
    "### テキストの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5070da80-dd10-40a9-9964-de098985474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# テキストの読み込み\n",
    "loader = TextLoader(\"output_wikipedia.txt\") # 取得してきたファイルを指定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cddba-41c5-432c-a5bd-bb0de6df9b68",
   "metadata": {},
   "source": [
    "### DocumentLoader\n",
    "- まずデータの読み込みに使えるツールがDocumentLoaderです\n",
    "  - https://python.langchain.com/docs/integrations/document_loaders/\n",
    "- Github, Slack, Notion, Excel, PDFなどさまざまな形式のファイルを読み込むための方法が提供されている\n",
    "- 今回は単純なtxtファイルなのでTextLoaderを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740fcc4-f8ed-411b-8768-8d4883ca23fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実際に読み込まれたか確認\n",
    "raw_docs = loader.load()\n",
    "print(\"ドキュメントの数:\", len(raw_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce76b082-35a7-4b46-adff-2e4556bca3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6789ddc-ef8b-4b7f-ab09-ffa40b7b1de0",
   "metadata": {},
   "source": [
    "### テキストの分割\n",
    "- ドキュメントをある程度の長さでチャンクに分割する\n",
    "  - チャンク：分割したテキストの1つ１つのこと\n",
    "- ドキュメントを適切な大きさのチャンクに分割することで、LLMに入力するトークン数を削減したり、より正確な回答を得やすくなる場合がある\n",
    "- 今回はCharacterTextSplitterを利用して先ほどの1つの文章を複数に分割していく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fefad-ad0d-4ef9-9a69-3b0d801b01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(raw_docs)\n",
    "print(\"ドキュメントの数:\", len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54a439-3cff-4060-bf44-fa1fca98ef24",
   "metadata": {},
   "source": [
    "- 元々1個だったドキュメントが27個に分割された\n",
    "- LangChainでは他にもソースコードをクラスや関数のようなまとまりでよしなに分割してくれる機能も提供されている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dea637b-baa7-4351-80a8-9fd07b8f64c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0番目のドキュメント\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fd5eff-abff-40c3-8b4c-e51742daf76e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1番目のドキュメント\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78076702-77d8-4312-91ff-85f41a37b008",
   "metadata": {},
   "source": [
    "### テキストのベクトル化（ベクトルストアを作成する）\n",
    "- ここではOpenAIのEmbedding APIを使いテキストをベクトル化します\n",
    "- モデルはtext-embedding-3-smallを利用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924bc73b-e613-441e-a872-e0689f03f6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Open AIのEmbedding APIをラップしたOpenAIEmbeddingsクラス\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac2e36-f1e6-40e2-9ec7-5c3aaf94447e",
   "metadata": {},
   "source": [
    "### 実際にEmbedding modelを利用してテキストのベクトル化を試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41303032-aca8-4725-b4bf-aa5995ee6526",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"秩序の街とはどのような街でしょうか\"\n",
    "vector = embeddings.embed_query(query) # Open AI APIへリクエストが行われる\n",
    "print(\"次元数: \", len(vector))\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5447ff1-ec90-4e1c-9183-6beaa6eebecd",
   "metadata": {},
   "source": [
    "- 1536というベクトルの次元数は「text-embedding-3-small」というモデルの設計時に決まっている\n",
    "- 次元が高いほど、多くの情報を保持できるが、計算コストも増えるのでバランスをとって1536次元になっているらしい"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abfd8c8-b846-4fa4-880a-364f87f5ab4e",
   "metadata": {},
   "source": [
    "### ベクトルストアの作成\n",
    "- 今回はローカルで気軽に利用できるベクトルストアとしてChromaを利用する\n",
    "- ベクトルストアの作成は簡単にできるようになっており、ドキュメントをベクトルストアに保存する際に内部で勝手にベクトル化される\n",
    "- ここでもベクトル化にはOpen AI APIへリクエストが行われる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4da627b-3572-49b4-8478-753bc1514212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\", # ローカルの保存場所\n",
    ")\n",
    "\n",
    "# ベクトル化はVector storeのクラスにデータを保存する際に内部的に実行される\n",
    "vector_store.add_documents(documents=docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8621df-c4ce-4424-8649-0902450e4e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector storeに保存されたベクトルを確認してみる\n",
    "\n",
    "# idの確認\n",
    "doc1_id = vector_store.get(include=['embeddings', 'documents'])['ids'][0]\n",
    "print(doc1_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d324fd-b627-4463-a0fb-c206145a4f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの値を確認\n",
    "doc1_vector = vector_store.get(include=['embeddings', 'documents'])['embeddings'][0] # getでembeddingsはデフォルトで除外されるので、includeで指定する\n",
    "\n",
    "print(\"次元数: \", len(doc1_vector))\n",
    "print(doc1_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4342d88a-2251-4cac-b4d1-070829de0f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を確認\n",
    "doc1 = vector_store.get(include=['embeddings', 'documents'])['documents'][0]\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeed840-3029-43b8-bd9e-0973e7192e07",
   "metadata": {},
   "source": [
    "## 2. 検索 (Retrieval)\n",
    "### retrieverを作成\n",
    "- ベクトルストアから関連するドキュメントを得るインターフェースを「retriever」と呼ぶ（検索をしてくれるもの）\n",
    "- ベクトルストアのインスタンスからretrieverの作成はLangChain側で簡単にできるようになっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b305e-a47f-4be6-b5ee-c3b8805cf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # 最大3件の検索結果を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e38c1-2bc4-4833-9999-0d5b84610481",
   "metadata": {},
   "source": [
    "### 試しに質問に近い文章をベクトルストアから検索してみよう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45459f-dc78-42c8-abdd-c9b93c9b16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 質問に近いドキュメントを検索\n",
    "query = \"スミナガシートに関して教えて\"\n",
    "\n",
    "context_docs = retriever.invoke(query)\n",
    "print(\"検索した結果のドキュメント数: \", len(context_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a583aa6-280d-4596-a4cb-5509a0c14bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd787c3-50c6-47cf-afba-2e80e1dc4902",
   "metadata": {},
   "source": [
    "## 3. 生成 (Generation)\n",
    "### 検索結果を元にAIに回答を生成させる\n",
    "- 検索結果をプロンプトに埋め込みLLMに質問して回答をもらう\n",
    "- まず完成形を見せて、順番に解説します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f0e8dc-eb59-4abb-8d84-9d58c2ded827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8741ece5-45b1-41e1-ad36-4bb56d8474d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "\n",
    "output = chain.invoke(\"秩序の街とはどのような街でしょうか\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083d2443-ce46-48a4-a71a-04cb5684f9e8",
   "metadata": {},
   "source": [
    "### LCEL(LangChain Expression Language)について\n",
    "- 一連の処理をLCELの記法で実装\n",
    "- Runnableなクラスを「｜」で繋ぐことで新たなRunnableを作り、invokeしたときに内部のRunnableが順番にinvokeされる\n",
    "\n",
    "```\n",
    "chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    ")\n",
    "```\n",
    "\n",
    "- LCELの記法に関して理解するため順番にinvokeしてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677049f3-a591-4d88-b187-4c00e5cb72d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_value = prompt.invoke({\"context\": \"テストの文脈です。明日の天気は晴れです。\", \"question\": \"明日の天気は何ですか？\"})\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8c74b7-b39a-4447-8b0d-43e74ef1c07a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ai_messsage = model.invoke(prompt_value)\n",
    "print(ai_messsage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277699d3-dae2-47f9-8c9a-b04cf5b8f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "output = output_parser.invoke(ai_messsage)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d608a-efbe-4141-b222-b864cb096399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
